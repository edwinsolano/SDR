# -*- coding: utf-8 -*-
"""Clasificador_LoRa

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_GYt7pJMJj7SeF2cabg7HMfFJdpKIWkW

## ESTABLECIMIENTO DEL DATASET
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pickle
import numpy as np
from sklearn import preprocessing
from sklearn.decomposition import PCA
import random
import tensorflow.keras.utils
import tensorflow.keras.models as models
from tensorflow.keras.layers import Reshape,Dense,Dropout,Activation,Flatten
from tensorflow.keras.layers import GaussianNoise
from tensorflow.keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.regularizers import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow.keras
import joblib  # For saving and loading model
import time

#import zipfile
#zip_ref = zipfile.ZipFile('/content/drive/MyDrive/Datasets/RML2016.10a.zip', 'r')
#zip_ref.extractall('/content/data')
#zip_ref.close()

Xd = pickle.load(open("/content/drive/MyDrive/SDR/dataset1-128-sf7-al-12.pkl",'rb'),encoding = "bytes")
snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])
X = []
lbl = []
for mod in mods:
    for snr in snrs:
        X.append(Xd[(mod,snr)])
        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))
X = np.vstack(X)

print(X)

import pickle
import numpy as np
import codecs

with open('/content/drive/MyDrive/SDR/dataset1-128-sf7-al-12.pkl','rb') as f:
  data = pickle.load(f, encoding='bytes')

print(data)
print(type(data))

# Partition the data
#  into training and test sets of the form we can train/test on
np.random.seed(2020)
n_examples = X.shape[0]
#n_train = n_examples // 2
n_train = int(n_examples * 0.7)
train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)
test_idx = list(set(range(0,n_examples))-set(train_idx))
X_train = X[train_idx]
X_test =  X[test_idx]

print(train_idx)

print(test_idx)

print(X_train.shape)
print(X_test.shape)

from sklearn.preprocessing import LabelBinarizer

# Ejemplo de etiquetas en formato b'SF7'
labels_bytes = [b'SF7', b'SF8', b'SF9', b'SF10']

# Decodificar etiquetas de bytes a cadenas
labels_strings = [label.decode('utf-8') for label in labels_bytes]

# Inicializar y ajustar LabelBinarizer
lb = LabelBinarizer()
one_hot_labels = lb.fit_transform(labels_strings)

# Resultado
print(one_hot_labels)

# Decodificar etiquetas de bytes a cadenas
#decoded_labels = [label.decode('utf-8') for label in np.asarray(lbl)[:, 0]]
#decoded_labels = np.asarray(lbl)[:, 0].astype(str)
#one-hot encoding the label
lb = preprocessing.LabelBinarizer()
lb.fit(np.asarray(lbl)[:,0])
print(lb.classes_)
lbl_encoded=lb.transform(np.asarray(lbl)[:,0])
#lbl_encoded=lb.transform(decoded_labels)
y_train=lbl_encoded[train_idx]
y_test=lbl_encoded[test_idx]

print(lbl_encoded)
print(lbl_encoded.shape)
print(y_train.shape)
print(y_test.shape)
print(np.asarray(lbl)[:,0])

in_shp = list(X_train.shape[1:])
print(X_train.shape, in_shp)
classes = mods

np.max(X_train), np.min(X_train)

"""## CONSTRUCCIÓN DEL MODELO DE RED NEURONAL"""

dr = 0.5 # dropout rate (%)
model = models.Sequential()
model.add(Reshape(in_shp + [1], input_shape = in_shp))

model.add(Conv2D(64, (1, 3), activation ='relu'))
model.add(Dropout(dr))

model.add(Conv2D(16, (2, 3), activation ='relu'))
model.add(Dropout(dr))

#model.add(Conv2D(16, (1, 4), activation ='relu'))
#model.add(Dropout(dr))

model.add(Flatten())
model.add(Dense(512, activation ='relu')) # 128
model.add(Dropout(dr))

model.add(Dense(128, activation ='relu')) # 128
model.add(Dropout(dr))

model.add(Dense(len(classes), activation ='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.summary()

tic = time.time()
history = model.fit(X_train, y_train, batch_size = 1024, epochs = 20, verbose = 1, validation_data = (X_test, y_test),
                    callbacks = [EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 1,
                                               mode = 'auto', restore_best_weights = True)])
toc = time.time()
training_time = toc - tic

print ("The training time is %.3f seconds" %(training_time))

"""## EVALUAR Y GRAFICAR EL RENDIMIENTO DEL MODELO"""

score = model.evaluate(X_test, y_test, batch_size=1024)
# score = model.evaluate(X_test_pca, y_test)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# Show loss curves
plt.figure(figsize = (11, 7))
plt.title('Rendimiento del entrenamiento')
plt.plot(history.epoch, history.history['loss'], label = 'train loss+error')
plt.plot(history.epoch, history.history['val_loss'], label = 'val_error')
plt.xlabel("Época")
plt.ylabel("Pérdida")
plt.legend()

from sklearn.metrics import classification_report
import numpy as np

y_predict = model.predict(X_test)
y_predict_classes = np.argmax(y_predict, axis=1)

print(classification_report(np.argmax(y_test, axis=1), y_predict_classes))

from sklearn.metrics import classification_report
y_predict = model.predict(X_test)
print(classification_report(np.argmax(y_test, axis = 1), np.argmax(y_predict, axis = 1)))

def plot_confusion_matrix(cm, title='Matriz de confusión', cmap = plt.cm.Blues, labels = []):
#     plt.figure(figsize = (5, 5))
    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation = 45)
    plt.yticks(tick_marks, labels)
    plt.tight_layout()
    plt.ylabel('Etiqueta real')
    plt.xlabel('Etiqueta predicha')
def confusion_matrix(model, classes, X_test, y_test):
    test_Y_hat = model.predict(X_test, batch_size = 1024)
    conf = np.zeros([len(classes), len(classes)])
    confnorm = np.zeros([len(classes), len(classes)])

    for i in range(0, X_test.shape[0]):
        j = list(y_test[i,:]).index(1)
        k = int(np.argmax(test_Y_hat[i,:]))
        conf[j,k] = conf[j,k] + 1

    for i in range(0,len(classes)):
        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])

    plot_confusion_matrix(confnorm, labels = classes)

# GRAFICAR LA MATRIZ DE CONFUSION
confusion_matrix(model, classes, X_test, y_test)

## Obtener la precisión de la prueba para diferentes SNRs
def getTestAccArray(model, lbl, X_test, y_test, test_idx):
    acc = []

    snr_array=np.asarray(lbl)[:,1]
    lb_temp = preprocessing.LabelBinarizer()
    lb_temp.fit(snr_array)
    temp_array=lb_temp.classes_
    snr_label_array = []
    test_SNRs=snr_array[test_idx]

    snr_label_array.append(temp_array[0])

    for snr in snr_label_array:
#         test_SNRs = map(lambda x: lbl[x][1], test_idx)
        test_X_i = X_test[np.where(test_SNRs==snr)]
        test_Y_i = y_test[np.where(test_SNRs==snr)]

        test_Y_i_hat = model.predict(test_X_i)
        conf = np.zeros([len(classes), len(classes)])
        confnorm = np.zeros([len(classes), len(classes)])

        for i in range(0, test_X_i.shape[0]):
            j = list(test_Y_i[i,:]).index(1)
            k = int(np.argmax(test_Y_i_hat[i,:]))
            conf[j,k] = conf[j,k] + 1

        for i in range(0, len(classes)):
            confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])

        plt.figure()
        SNR_value = snr.astype('int')
        plot_confusion_matrix(confnorm, labels=classes, title="Matriz de confusión ConvNet (SNR=%d)"%(SNR_value))

        cor = np.sum(np.diag(conf))
        ncor = np.sum(conf) - cor
        print("Precisión general:", cor / (cor+ncor),"para SNR",snr)
#         acc[snr] = 1.0*cor/(cor+ncor)
        acc.append(1.0*cor/(cor+ncor))

    return acc

acc = getTestAccArray(model, lbl, X_test, y_test, test_idx)

# Plot accuracy curve
plt.figure(figsize = (11, 7))
plt.plot(snrs, acc)
plt.xlabel("Nivel de SNR (dB)")
plt.ylabel("% de precisión de clasificación")
plt.title('Precisión vs SNR')
